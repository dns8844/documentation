
== Under the hood

=== Overview

Raspberry Pi AI相机的工作方式与传统的基于AI的相机图像处理系统不同，如下图所示：

image::images/imx500-comparison.svg[Traditional versus IMX500 AI camera systems]

左侧演示了传统人工智能相机系统的架构。在这样的系统中，相机将图像传送到树莓派。树莓派处理图像，然后执行人工智能推理。传统系统可能使用外部人工智能加速器（如图所示）或完全依赖中央处理器。

右侧演示了使用IMX500的系统架构。相机模块包含一个小型图像信号处理器（ISP），它将原始相机图像数据转换为 **输入张量(input tensor)**。相机模块将此张量直接发送到相机内的AI加速器，该加速器产生包含推理结果的 **输出张量(output tensors)**。AI加速器将这些张量发送到树莓派。不需要外部加速器，也不需要树莓派在CPU上运行神经网络软件。

要完全理解这个系统，请熟悉以下概念：

输入张量(Input Tensor):: 传递给人工智能引擎进行推理的传感器图像部分。由小型板载ISP生成，该ISP还将相机图像裁剪和缩放到已加载的神经网络预期的尺寸。输入张量通常不适用于应用程序，尽管可以出于调试目的访问它。

感兴趣区域（Region of Interest ROI）:: 指定传感器图像的哪一部分在重新缩放到神经网络要求的大小之前被裁剪。可以由应用程序查询和设置。使用的单位始终是全分辨率传感器输出中的像素。默认ROI设置使用从传感器接收的完整图像，不裁剪任何数据。

输出张量(Output Tensors):: 由神经网络执行的推理结果。输出的精确数量和形状取决于神经网络。应用程序代码必须了解如何处理张量。

=== System architecture

下图显示了在我们使用Raspberry Pi AI相机模块硬件（红色）进行成像/推理用例期间使用的各种相机软件组件（绿色）：

image::images/imx500-block-diagram.svg[IMX500 block diagram]

在启动时，IMX500传感器模块加载固件以运行特定的神经网络模型，在流式传输期间，IMX500 _同时_ 生成图像流和推理流，此推理流保存神经网络模型的输入和输出，也称为输入/输出 **张量(tensors)**。

=== Device drivers

在最低级别，IMX500传感器内核驱动程序通过I2C总线配置相机模块。CSI2驱动程序（Pi 5上的 `CFE`，所有其他Pi平台上的 `Unicam`）设置接收器将图像数据流写入帧缓冲区，以及嵌入数据和推理数据流写入内存中的另一个缓冲区。

固件文件也通过I2C总线传输。在大多数设备上，这使用标准的I2C协议，但树莓派5使用自定义高速协议。内核中的RP2040 SPI驱动程序处理固件文件传输，因为传输使用RP2040微控制器。微控制器通过SPI总线将I2C传输从内核桥接到IMX500。此外，RP2040将固件文件缓存在板载存储中。这避免了通过I2C总线传输整个固件blob的需要，显着加快了已经使用过固件的加载速度。

=== `libcamera`

一旦 `libcamera` 从内核中取出图像和推理数据缓冲区，IMX500特定的 `cam-helper` 库（`libcamera` 中Raspberry Pi IPA的一部分）就会解析推理缓冲区以访问输入/输出张量。这些张量被打包为Raspberry Pi供应商特定的 https://libcamera.org/api-html/namespacelibcamera_1_1controls.html[`libcamera` 控件]。`libcamera` 返回以下控件：

[%header,cols="a,a"]
|===
| Control
| Description

| `CnnOutputTensor`
| Floating point array storing the output tensors.

| `CnnInputTensor`
| Floating point array storing the input tensor.

| `CnnOutputTensorInfo`
| Network specific parameters describing the output tensors' structure:

[source,c]
----
struct OutputTensorInfo {
	uint32_t tensorDataNum;
	uint32_t numDimensions;
	uint16_t size[MaxNumDimensions];
};

struct CnnOutputTensorInfo {
	char networkName[NetworkNameLen];
	uint32_t numTensors;
	OutputTensorInfo info[MaxNumTensors];
};
----

| `CnnInputTensorInfo`
| Network specific parameters describing the input tensor's structure:

[source,c]
----
struct CnnInputTensorInfo {
	char networkName[NetworkNameLen];
	uint32_t width;
	uint32_t height;
	uint32_t numChannels;
};
----

|===

=== `rpicam-apps`

`rpicam-apps` 提供了一个IMX500后处理阶段基类，实现了IMX500后处理阶段的功能：https://github.com/raspberrypi/rpicam-apps/blob/main/post_processing_stages/imx500/imx500_post_processing_stage.hpp[`IMX500PostProcessingStage`].  使用该基类可为在 IMX500 上运行的任何神经网络模型派生一个新的后处理阶段。示例见 https://github.com/raspberrypi/rpicam-apps/blob/main/post_processing_stages/imx500/imx500_object_detection.cpp[`imx500_object_detection.cpp`]：

[source,cpp]
----
class ObjectDetection : public IMX500PostProcessingStage
{
public:
	ObjectDetection(RPiCamApp *app) : IMX500PostProcessingStage(app) {}

	char const *Name() const override;

	void Read(boost::property_tree::ptree const &params) override;

	void Configure() override;

	bool Process(CompletedRequestPtr &completed_request) override;
};
----

对于应用程序接收到的每一帧，都会调用 `Process()` 函数（在上述情况下为 `ObjectDetection::Process()`）。在此函数中，您可以提取输出张量以进行进一步处理或分析：

[source,cpp]
----
auto output = completed_request->metadata.get(controls::rpi::CnnOutputTensor);
if (!output)
{
  LOG_ERROR("No output tensor found in metadata!");
  return false;
}

std::vector<float> output_tensor(output->data(), output->data() + output->size());
----

完成后，最终结果可以可视化或保存在元数据中，并由另一个下游阶段或顶级应用程序本身使用。在对象推理案例中：

[source,cpp]
----
if (objects.size())
  completed_request->post_process_metadata.Set("object_detect.results", objects);
----

下游运行的 `object_detect_draw_cv` 后处理阶段从元数据中获取这些结果，并在 `ObjectDetectDrawCvStage::Process()` 函数中将边界框绘制到图像上：

[source,cpp]
----
std::vector<Detection> detections;
completed_request->post_process_metadata.Get("object_detect.results", detections);
----

下表包含 `IMX500PostProcessingStage` 提供的帮助函数的完整列表：

[%header,cols="a,a"]
|===
| Function
| Description

| `Read()`
| Typically called from `<Derived Class>::Read()`, this function reads the config parameters for input tensor parsing and saving.

This function also reads the neural network model file string (`"network_file"`) and sets up the firmware to be loaded on camera open.

| `Process()`
| Typically called from `<Derived Class>::Process()` this function processes and saves the input tensor to a file if required by the JSON config file.

| `SetInferenceRoiAbs()`
| Sets an absolute region of interest (ROI) crop rectangle on the sensor image to use for inferencing on the IMX500.

| `SetInferenceRoiAuto()`
| Automatically calculates region of interest (ROI) crop rectangle on the sensor image to preserve the input tensor aspect ratio for a given neural network.

| `ShowFwProgressBar()`
| Displays a progress bar on the console showing the progress of the neural network firmware upload to the IMX500.

| `ConvertInferenceCoordinates()`
| Converts from the input tensor coordinate space to the final ISP output image space.

There are a number of scaling/cropping/translation operations occurring from the original sensor image to the fully processed ISP output image. This function converts coordinates provided by the output tensor to the equivalent coordinates after performing these operations.

|===

=== Picamera2

Picamera2中的IMX500集成与 `rpicam-apps` 中的集成非常相似。Picamera2有一个IMX500帮助类，它提供与 `rpicam-apps` `IMX500PostProcessingStage` 基类相同的功能。这可以通过以下方式导入任何Python脚本：

[source,python]
----
from picamera2.devices.imx500 import IMX500

# This must be called before instantiation of Picamera2
imx500 = IMX500(model_file)
----

要检索输出张量，请从控件中获取它们。然后，您可以在Python脚本中应用额外的处理。

例如，在 https://github.com/raspberrypi/picamera2/tree/main/examples/imx500/imx500_object_detection_demo.py[imx500_object_detection_demo.py] 等对象推理用例中，在 `parse_detections()` 中提取对象边界框和置信度值，并在 `draw_detections()` 中在图像上绘制框：

[source,python]
----
class Detection:
    def __init__(self, coords, category, conf, metadata):
        """Create a Detection object, recording the bounding box, category and confidence."""
        self.category = category
        self.conf = conf
        obj_scaled = imx500.convert_inference_coords(coords, metadata, picam2)
        self.box = (obj_scaled.x, obj_scaled.y, obj_scaled.width, obj_scaled.height)

def draw_detections(request, detections, stream="main"):
    """Draw the detections for this request onto the ISP output."""
    labels = get_labels()
    with MappedArray(request, stream) as m:
        for detection in detections:
            x, y, w, h = detection.box
            label = f"{labels[int(detection.category)]} ({detection.conf:.2f})"
            cv2.putText(m.array, label, (x + 5, y + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
            cv2.rectangle(m.array, (x, y), (x + w, y + h), (0, 0, 255, 0))
        if args.preserve_aspect_ratio:
            b = imx500.get_roi_scaled(request)
            cv2.putText(m.array, "ROI", (b.x + 5, b.y + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)
            cv2.rectangle(m.array, (b.x, b.y), (b.x + b.width, b.y + b.height), (255, 0, 0, 0))

def parse_detections(request, stream='main'):
    """Parse the output tensor into a number of detected objects, scaled to the ISP output."""
    outputs = imx500.get_outputs(request.get_metadata())
    boxes, scores, classes = outputs[0][0], outputs[1][0], outputs[2][0]
    detections = [ Detection(box, category, score, metadata)
                   for box, score, category in zip(boxes, scores, classes) if score > threshold]
    draw_detections(request, detections, stream)
----

与 `rpicam-apps` 示例不同，此示例不应用额外的滞后或时间过滤。

Picamera2中的IMX500类提供了以下帮助函数：

[%header,cols="a,a"]
|===
| Function
| Description

| `IMX500.get_full_sensor_resolution()`
| Return the full sensor resolution of the IMX500.

| `IMX500.config`
| Returns a dictionary of the neural network configuration.

| `IMX500.convert_inference_coords(coords, metadata, picamera2)`
| Converts the coordinates _coords_ from the input tensor coordinate space to the final ISP output image space. Must be passed Picamera2's image metadata for the image, and the Picamera2 object.

There are a number of scaling/cropping/translation operations occurring from the original sensor image to the fully processed ISP output image. This function converts coordinates provided by the output tensor to the equivalent coordinates after performing these operations.

| `IMX500.show_network_fw_progress_bar()`
| Displays a progress bar on the console showing the progress of the neural network firmware upload to the IMX500.

| `IMX500.get_roi_scaled(request)`
| Returns the region of interest (ROI) in the ISP output image coordinate space.

| `IMX500.get_isp_output_size(picamera2)`
| Returns the ISP output image size.

| `IMX5000.get_input_size()`
| Returns the input tensor size based on the neural network model used.

| `IMX500.get_outputs(metadata)`
| Returns the output tensors from the Picamera2 image metadata.

| `IMX500.get_output_shapes(metadata)`
| Returns the shape of the output tensors from the Picamera2 image metadata for the neural network model used.

| `IMX500.set_inference_roi_abs(rectangle)`
| Sets the region of interest (ROI) crop rectangle which determines which part of the sensor image is converted to the input tensor that is used for inferencing on the IMX500. The region of interest should be specified in units of pixels at the full sensor resolution, as a `(x_offset, y_offset, width, height)` tuple.

| `IMX500.set_inference_aspect_ratio(aspect_ratio)`
| Automatically calculates region of interest (ROI) crop rectangle on the sensor image to preserve the given aspect ratio. To make the ROI aspect ratio exactly match the input tensor for this network, use `imx500.set_inference_aspect_ratio(imx500.get_input_size())`.

| `IMX500.get_kpi_info(metadata)`
| Returns the frame-level performance indicators logged by the IMX500 for the given image metadata.

|===
